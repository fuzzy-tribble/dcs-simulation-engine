{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29bb1e42",
   "metadata": {},
   "source": [
    "# Demo Evaluate New Games and/or Characters\n",
    "\n",
    "**Purpose**: The purpose of this notebook is to provide an example/demo/copy-pasteable template for running analysis using the simulation engine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb05876",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "These instruction assume you are doing in this using the devcontainer extention in VSCode (if you haven't done that check the Contributing.md file in the main repo for instructions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914a3cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation engine core modules\n",
    "from dcs_simulation_engine.core.run_manager import RunManager\n",
    "from dcs_simulation_engine.core.game_config import GameConfig\n",
    "import dcs_simulation_engine.helpers.database_helpers as dbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74eb8744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hid': 'human-new', 'short_description': 'A new human.', 'inclusion_reason': 'To represent ....', 'common_descriptors': ['human', 'biological', 'visually-impaired', 'human-like-cognition'], 'anthronormal_divergence': {'Sensory-divergence': 'Reduced visual acuity, field, or clarity.'}, 'interaction_examples': None, 'long_description': 'I can perceive light, shapes, motion, and sometimes text or detail, but my vision is limited compared to typical humans. Depending on the cause, I may have blurred central vision, reduced peripheral vision, low contrast sensitivity, impaired depth perception, or difficulty detecting fine detail. My visual capacity might fluctuate depending on lighting, fatigue, or assistive tools.\\n\\nI often rely on enlarged text, high-contrast visuals, audio descriptions, tactile feedback, or assistive technologies such as screen readers or magnifiers. In unfamiliar or visually complex environments, I use memory, mobility aids (like canes), or guidance from others. My cognition, reasoning, and emotions are typical of humans, though the strategies I use to navigate and interpret visual information differ.', 'abilities': {'Sensory / Perceptual': ['Can detect light, motion, and general shapes depending on severity and type of impairment.', 'Can sometimes read large/high-contrast text or use magnification tools.', 'Cannot reliably perceive fine visual detail or distant objects without assistive tools.', 'May struggle with low-light or visually cluttered environments.', 'May use auditory, tactile, or memory-based cues for navigation and information gathering.'], 'Perceptual / Cognitive Processing': ['Can process visual information but with reduced resolution or incomplete input.', 'Can combine memory, touch, and hearing to compensate for gaps in visual data.', 'Cannot rely on fast visual scanning or detail recognition the way typical humans do.'], 'Regulatory': ['Normal emotional and physiological regulation.', 'May experience eye strain, headaches, or fatigue when visually focusing for long periods.'], 'Action / Motor': ['Can perform most motor tasks but may navigate space more slowly or with mobility aids.', 'Cannot safely perform actions requiring high visual precision without assistance (e.g. driving).']}}\n"
     ]
    }
   ],
   "source": [
    "# Load the json with the new character definition\n",
    "import json\n",
    "\n",
    "file_path = \"new-character.json\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    new_character_dict = json.load(f)\n",
    "\n",
    "print(new_character_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1469b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-07 14:26:35.097\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mhelpers\u001b[0m:\u001b[36m_seed_from_dir\u001b[0m:\u001b[36m70\u001b[0m - \u001b[34m\u001b[1mSeeding 9 docs into collection 'characters' from characters.json to run tests.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:35.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mhelpers\u001b[0m:\u001b[36m_seed_from_dir\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1mNo documents found in players.json; skipping.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:35.099\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mhelpers\u001b[0m:\u001b[36m_seed_from_dir\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1mNo documents found in runs.json; skipping.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Make sure we use a mock database for running tests\n",
    "from helpers import use_mongomock_for_db\n",
    "\n",
    "use_mongomock_for_db(seed_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fc928dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the database and make sure its mongomock (not real)\n",
    "db = dbh.get_db()\n",
    "\n",
    "import mongomock\n",
    "\n",
    "assert isinstance(db.client, mongomock.MongoClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425cdda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human-normative',\n",
       " 'human-low-vision',\n",
       " 'human-non-hearing',\n",
       " 'human-non-ambulatory',\n",
       " 'human-multi-divergent-complex',\n",
       " 'llm-gpt5',\n",
       " 'flatworm',\n",
       " 'thermostat',\n",
       " 'algorithm-sort']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List character hids from the database\n",
    "hids = [doc.get(\"hid\") for doc in db[\"characters\"].find({}, {\"hid\": 1, \"_id\": 0})]\n",
    "hids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c741dbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['human-normative',\n",
       " 'human-low-vision',\n",
       " 'human-non-hearing',\n",
       " 'human-non-ambulatory',\n",
       " 'human-multi-divergent-complex',\n",
       " 'llm-gpt5',\n",
       " 'flatworm',\n",
       " 'thermostat',\n",
       " 'algorithm-sort',\n",
       " 'human-new']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add your new character\n",
    "db[\"characters\"].insert_one(new_character_dict)\n",
    "# List character hids again to see the new one added\n",
    "hids = [doc.get(\"hid\") for doc in db[\"characters\"].find({}, {\"hid\": 1, \"_id\": 0})]\n",
    "hids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac83043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure your new game is valid\n",
    "from pathlib import Path\n",
    "\n",
    "new_game_config = GameConfig.from_yaml(Path(\"new-game.yml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f4819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-07 14:26:38.781\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.run_manager\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m147\u001b[0m - \u001b[34m\u001b[1mCreate class method called with game=name='New Game Demo' description='A demo\\n' version='1.0.0' authors=['DCS'] stopping_conditions={'runtime_seconds': ['>=300'], 'turns': ['>=10']} state_overrides={} access_settings=AccessSettings(user=ValiditySelector(valid={'players': {}}, invalid={}), consent_form={}) data_collection_settings={'save_runs': False} character_settings=CharacterSettings(pc=ValiditySelector(valid={'characters': {'where': {'hid': 'human-normative'}}}, invalid={}), npc=ValiditySelector(valid={'characters': {'where': {'hid': 'human-new'}}}, invalid={})) graph_config=GraphConfig(name='demo-game-graph', description='A graph that defines the flow of the demo game.', state_overrides=None, nodes=[Node(name='command_filter', kind='builtin.command_filter', kwargs={'command_handlers': {'help': {'special_user_message': {'type': 'info', 'content': 'Describe an action...\\n- Eg. If your character can see and move you might say \"I look around the room and walk to the door.\"\\n\\nHere is are reminder of your character\\'s abilities:\\n{{ pc.abilities }}\\n'}}}}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='enter_message', kind='builtin.update_state', kwargs={'state_updates': {'special_user_message': {'type': 'info', 'content': '# Welcome\\nIn this game there is no predefined objective or task. You can just engage freely with the other character. \\n\\nYou are playing as: {{ pc.hid }} ({{ pc.short_description }})\\n\\nThe simulator is playing as: {{ npc.hid }} ({{ npc.short_description }})\\n'}, 'lifecycle': 'UPDATE'}}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='exit_message', kind='builtin.update_state', kwargs={'state_updates': {'special_user_message': {'type': 'info', 'content': 'Game exited with reason: {{ exit_reason }}\\n'}}}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='complete_message', kind='builtin.update_state', kwargs={'state_updates': {'special_user_message': {'type': 'info', 'content': '# Game Complete\\nThe game has completed after {{ len(events) }} total turns.\\n'}}}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='error_in_checkpoint', kind='builtin.raise_error', kwargs={'message': 'An unhandled checkpoint condiftion was reached...\\n- invalid_reason: {{ invalid_reason }}\\n- event_draft: {{ event_draft }}\\n- retries: {{ retries }}\\n- retry_limits: {{ retry_limits }}\\n'}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='error_in_lifecycle', kind='builtin.raise_error', kwargs={'message': 'An unhandled lifecycle status was reached: {{ lifecycle }}\\n'}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='user_max_retry_exit', kind='builtin.update_state', kwargs={'state_updates': {'lifecycle': 'EXIT', 'exit_reason': 'maximum retries exceeded'}}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='simulator_max_retry_exit', kind='builtin.update_state', kwargs={'state_updates': {'lifecycle': 'EXIT', 'exit_reason': 'simulator could not produce valid response within retry limit'}}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='retry', kind='builtin.retry', kwargs={'retry_message': 'Your previous action was invalid: Please try again.\\nRemaining Retries: {{ retry_limits[event_draft.type] - retries[event_draft.type] }}\\nPrevious Action: {{ event_draft.content }}\\nReason: {{ invalid_reason }}\\n'}, provider=None, model=None, system_template=None, additional_kwargs=None), Node(name='checkpoint', kind='custom', kwargs={}, provider='openrouter', model='openai/gpt-5-mini', system_template='You are a validator that decides whether the `{{ event_draft.type }}`\\'s proposed response is valid.\\n\\n{% if events|length == 0 %}\\nFIRST TURN:\\n1. MUST be an opening scene.\\n2. MUST begin with: **\"You enter a new space. In this space\"**\\n3. MUST be 2-3 sentences setting a shared scene where both characters could plausibly be present based on their descriptions and abilities.\\n\\n{% elif event_draft.type == \"user\" %}\\nUSER RESPONSE:\\n1. MUST describe plausible observable actions based on their character\\'s abilities. Repeating actions, leaving/returning to the scene or trying multiple times is allowed. For example, \\n  - if the user\\'s character can see, \"I look around ...\" is valid. \\n  - if the user\\'s character cannot hear, \"I listen for ...\" is invalid.\\n  - \"Help me calculate this...\" is invalid because it does not describe an observable action.\\n  - Internal states or conclusions like “I figure out…”, “I realize…” are never valid because they do not describe observable actions.\\n2. MUST NOT decide outcomes of their actions. For example,\\n  - “I look at the man. He looks back at me.” is invalid because it decides the man\\'s reaction.\\n  - \"I reach over tapping the table to try and get his attention.\" is valid because doesn\\'t decide if the action is successful.\\n4. MAY USE ANY OBJECT that could be present (EVEN IF NOT YET MENTIONED!!!). For example,\\n  - If the scene is a kitchen, and the user\\'s character has hands, they may say \"I pick up a knife from the counter\" even if knives were not previously mentioned.\\n  - However, they may NOT use or reference objects that are implausible in the context like a rocket launcher in a chemistry lab.\\n5. MAY leave the scene, walk away, etc. as long as they are within the character abilities.\\n\\n{% elif event_draft.type == \"ai\" %}\\nSIMULATOR RESPONSE:\\n0. Adjudicate the user\\'s last action: \\nAssume success if its within the user character abilities. Report the result of that action in the world. For example, if the user can see and they say \"I look around for a light switch\", the scene advancement should include something like: \"You see a light switch on the wall.\" If the user\\'s last action was leaving the scene, you may narrate the world and/or simulator character\\'s reaction to that if any.\\n1. Sense-bounded narration:\\nOnly narrate what the user\\'s character could presently perceive through their available senses. For example, if the user\\'s character can see, you may narrate visual details of the scene. If the user\\'s character cannot hear, do not narrate sounds.\\n2. Perception-bounded NPC behavior: \\nSimulator characters only react to things they have the ability to detect. If the user does something the user cannot perceive, do not response as if they perceived it; instead narrate what the simulator character is doing/sensing. For example:\\n  - If the user waves silently and the NPC is blind: do not wave back; instead, output something the blind NPC is doing or sensing at that moment.\\n  - If the user speaks and the NPC can hear: the NPC may respond verbally or behaviourally to the speech as appropriate.\\n  - If the user takes an unobservable internal action (“I think about…”): do not respond as if perceived; just continue with the NPC’s plausible next action.\\n3. No new user actions / no user internals:\\nDo not invent new actions for the user or narrate their thoughts/feelings. Only reflect outcomes of the action they actually took.\\n4. Continuity and feasibility\\nAll narration must remain physically/logically continuous within each characters abilities in context.\\n5. Single observable step:\\nAdvance the scene by one concrete, externally observable outcome (world or simulator character action) at a time. Do not jump ahead multiple steps or narrate future effects.\\n6. No unexpressed internals:\\nDo not narrate internal states (beliefs/motives/emotions) of any agent unless they are externally expressed through observable behaviour like speech or action. For example, stating an external observable like \"exploring a room...\" is valid, but \"feeling anxious...\" or \"using mechanosensation...\" is invalid unless the character expresses it through action like speaking.\\n{% endif %}\\n\\n----\\n{% if event_draft.type == \\'user\\' %}\\nUser/player character Abilities:\\n{{ pc.abilities }}\\n\\n{% elif event_draft.type == \\'ai\\' %}\\nSimulator/non-player character Abilities:\\n{{ npc.abilities }}\\n\\n{% endif %}\\n----\\n\\nActions so far: {% if events|length == 0 %} None {% else %}{{ events }}{% endif %}\\n\\nNext Proposed action:\\n{{ event_draft }}\\n\\nOutput format: {\\n    \"invalid_reason\": str?    # if invalid, provide reason, otherwise omit\\n    \"events\": [{            # if valid, copy the Next Proposed Action, otherwise omit\\n        \"type\": str?,         \\n        \"content\": str?        \\n      }],\\n  }\\n', additional_kwargs=None), Node(name='update', kind='custom', kwargs={}, provider='openrouter', model='openai/gpt-5-mini', system_template='You are the scene-advancer. The user controls their own character. You play only the simulator\\'s character (NPC). You must not speak or act for the user\\'s character.\\n- User\\'s character is: {{ pc.short_description }} ({{ pc._short_description }})\\n- User character abilities: {{ pc.abilities }}\\n\\n- Simulator\\'s character is: {{ npc.short_description }} ({{ npc._short_description }})\\n- Simulator character (your character) description: {{ npc.long_description }}\\n- Simulator character (your character) abilities: {{ npc.abilities }}\\n----\\nWhen advancing the scene:\\n\\n0. Adjudicate the user\\'s last action: \\nAssume success if its within the user character abilities. Report the result of that action in the world. For example, if the user can see and they say \"I look around for a light switch\", the scene advancement should include something like: \"You see a light switch on the wall.\"\\n\\n1. Sense-bounded narration:\\nOnly narrate what the user\\'s character could presently perceive through their available senses.\\n\\n2. Perception-bounded NPC behavior: \\nSimulator characters only react to things they have the ability to detect. If the user does something the user cannot perceive, do not response as if they perceived it; instead narrate what the simulator character is doing/sensing. For example:\\n  - If the user waves silently and the NPC is blind: do not wave back; instead, output something the blind NPC is doing or sensing at that moment.\\n  - If the user speaks and the NPC can hear: the NPC may respond verbally or behaviourally to the speech as appropriate.\\n  - If the user takes an unobservable internal action (“I think about…”): do not respond as if perceived; just continue with the NPC’s plausible next action.\\n  \\n3. No new user actions / no user internals:\\nDo not invent new actions for the user or narrate their thoguhts/feelings. Only reflect outcomes of the action they actually took.\\n\\n4. Continuity and feasibility\\nAll narration must remain physically/logically continuous within each characters abilities in context.\\n\\n5. Single observable step:\\nAdvance the scene by one concrete, externally observable outcome (world or simulator character action) at a time. Do not jump ahead multiple steps or narrate future effects.\\n\\n6. No unexpressed internals:\\nDo not narrate internal states (beliefs/motives/emotions) of any agent unless they are externally expressed through observable behaviour like speech or action.\\n{% if not events %}\\nDescribe a 1-2 sentence opening scene where both characters could plausibly be present, setting the stage for a potential interaction. It should start with \"You enter a new space. In this space...\".\\nFor example:\\n- If the user\\'s character has the ability to perceive textual/keyboard input, you could set up a computer/typing scene like \"You enter a new space. In this space, you sit in front of a keyboard and screen with a chat-like interface open.\"\\n\\nStart the opening scene now.\\n{% else %}\\nYour job is to advance the scene one step in response to the user\\'s last action.\\n\\nProvide a response to the last user action now.\\n{% endif %}\\n\\nActions so far: \\n\\n{% if events|length == 0 %} None {% else %} {{ events }}{% endif %}\\n{% if invalid_reason %}\\n{{ invalid_reason }}\\n\\nProvide a response to the last user action and ensure it follows the rules.\\n{% endif %}\\nWrite ONLY the scene output in the following JSON format — no meta-text, no explanations, no reasoning, no restatement of rules.\\nOutput format: {\\n  \"event_draft\": {\\n    \"type\": \"ai\",\\n    \"content\": str # a description of how the scene advances including any next actions taken by your NPC - no reasoning, explanations, or extra text.\\n  }\\n  \"invalid_reason\": null,          # don\\'t change this field\\n}\\n', additional_kwargs=None)], edges=[Edge(from_='__START__', to=ConditionalTo(conditional=[IfThen(if_=\"state['special_user_message']\", then='__END__'), IfThen(if_=\"state['lifecycle'] == 'ENTER'\", then='enter_message'), IfThen(if_=\"state['lifecycle'] == 'EXIT'\", then='exit_message'), IfThen(if_=\"state['lifecycle'] == 'UPDATE' and state['event_draft']['type'] == 'user'\", then='command_filter'), IfThen(if_=\"state['lifecycle'] == 'UPDATE' and state['event_draft']['type'] == 'ai'\", then='update'), ElseOnly(else_='error_in_lifecycle')])), Edge(from_='enter_message', to='update'), Edge(from_='exit_message', to='__END__'), Edge(from_='command_filter', to=ConditionalTo(conditional=[IfThen(if_=\"state['special_user_message']\", then='__END__'), ElseOnly(else_='checkpoint')])), Edge(from_='update', to='checkpoint'), Edge(from_='checkpoint', to=ConditionalTo(conditional=[IfThen(if_=\"state['invalid_reason'] and state['retries'][state['event_draft']['type']] < state['retry_limits'][state['event_draft']['type']]\", then='retry'), IfThen(if_=\"state['event_draft']['type'] == 'user' and state['invalid_reason'] and state['retries'][state['event_draft']['type']] >= state['retry_limits'][state['event_draft']['type']]\", then='user_max_retry_exit'), IfThen(if_=\"state['event_draft']['type'] == 'ai' and state['invalid_reason'] and state['retries'][state['event_draft']['type']] >= state['retry_limits'][state['event_draft']['type']]\", then='simulator_max_retry_exit'), IfThen(if_=\"state['event_draft']['type'] == 'ai'\", then='__END__'), IfThen(if_=\"state['event_draft']['type'] == 'user'\", then='update'), ElseOnly(else_='error_in_checkpoint')])), Edge(from_='user_max_retry_exit', to='exit_message'), Edge(from_='simulator_max_retry_exit', to='exit_message'), Edge(from_='retry', to=ConditionalTo(conditional=[IfThen(if_=\"{{ state['event_draft']['type'] == 'user' }}\", then='__END__'), IfThen(if_=\"{{ state['event_draft']['type'] == 'ai' }}\", then='update'), ElseOnly(else_='error_in_checkpoint')]))])\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.game_config\u001b[0m:\u001b[36mis_player_allowed\u001b[0m:\u001b[36m212\u001b[0m - \u001b[34m\u001b[1mis_player_allowed: valid map has empty query -> allow\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.784\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.game_config\u001b[0m:\u001b[36mis_player_allowed\u001b[0m:\u001b[36m247\u001b[0m - \u001b[34m\u001b[1mis_player_allowed: player_id=None -> valid=True invalid=False allowed=True\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.786\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.helpers.database_helpers\u001b[0m:\u001b[36mlist_characters_where\u001b[0m:\u001b[36m329\u001b[0m - \u001b[34m\u001b[1mListing character hids from collection=characters with query={'where': {'hid': 'human-normative'}}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.788\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.helpers.database_helpers\u001b[0m:\u001b[36mlist_characters_where\u001b[0m:\u001b[36m364\u001b[0m - \u001b[34m\u001b[1mFound 1 documents matching.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.789\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.game_config\u001b[0m:\u001b[36mget_valid_characters\u001b[0m:\u001b[36m169\u001b[0m - \u001b[34m\u001b[1mPCs: |V|=1 |I|=0 |V-I|=1\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.helpers.database_helpers\u001b[0m:\u001b[36mlist_characters_where\u001b[0m:\u001b[36m329\u001b[0m - \u001b[34m\u001b[1mListing character hids from collection=characters with query={'where': {'hid': 'human-new'}}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.791\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.helpers.database_helpers\u001b[0m:\u001b[36mlist_characters_where\u001b[0m:\u001b[36m364\u001b[0m - \u001b[34m\u001b[1mFound 1 documents matching.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.793\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.game_config\u001b[0m:\u001b[36mget_valid_characters\u001b[0m:\u001b[36m177\u001b[0m - \u001b[34m\u001b[1mNPCs: |V|=1 |I|=0 |V-I|=1\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.795\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.helpers.database_helpers\u001b[0m:\u001b[36mget_character_from_hid\u001b[0m:\u001b[36m408\u001b[0m - \u001b[34m\u001b[1mLoading character by hid='human-normative' from collection='characters'\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.helpers.database_helpers\u001b[0m:\u001b[36mget_character_from_hid\u001b[0m:\u001b[36m408\u001b[0m - \u001b[34m\u001b[1mLoading character by hid='human-new' from collection='characters'\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.797\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.state\u001b[0m:\u001b[36mmake_state\u001b[0m:\u001b[36m22\u001b[0m - \u001b[33m\u001b[1mNo lifecycle provided; defaulting to INIT.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.state\u001b[0m:\u001b[36mmake_state\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mNo retry_limits provided; defaulting to {'user': 3, 'ai': 3}.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.799\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.run_manager\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m224\u001b[0m - \u001b[34m\u001b[1mNo state overrides to apply.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.800\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.run_manager\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m227\u001b[0m - \u001b[34m\u001b[1mApplying stopping conditions from game config:                         {'runtime_seconds': ['>=300'], 'turns': ['>=10']}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.800\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.run_manager\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m236\u001b[0m - \u001b[34m\u001b[1mInitial state created: {'events': [], 'lifecycle': 'INIT', 'exit_reason': '', 'special_user_message': None, 'event_draft': None, 'invalid_reason': None, 'retries': {'user': 0, 'ai': 0}, 'retry_limits': {'user': 3, 'ai': 3}, 'forms': None}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.801\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.utils.chat\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[34m\u001b[1mInitializing ChatOpenRouter with parameters: {'model': 'openai/gpt-5-mini'}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.987\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.utils.chat\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[34m\u001b[1mInitializing ChatOpenRouter with parameters: {'model': 'openai/gpt-5-mini'}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.988\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mcompile\u001b[0m:\u001b[36m152\u001b[0m - \u001b[1mCompiling simulation graph...\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.989\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.state\u001b[0m:\u001b[36mmake_state\u001b[0m:\u001b[36m22\u001b[0m - \u001b[33m\u001b[1mNo lifecycle provided; defaulting to INIT.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.state\u001b[0m:\u001b[36mmake_state\u001b[0m:\u001b[36m24\u001b[0m - \u001b[34m\u001b[1mNo retry_limits provided; defaulting to {'user': 3, 'ai': 3}.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.989\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.context\u001b[0m:\u001b[36mmake_context\u001b[0m:\u001b[36m23\u001b[0m - \u001b[34m\u001b[1mCreating default context with temp characters.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:38.990\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.helpers.database_helpers\u001b[0m:\u001b[36mget_character_from_hid\u001b[0m:\u001b[36m408\u001b[0m - \u001b[34m\u001b[1mLoading character by hid='human-normative' from collection='characters'\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:39.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36m_log_graph_debug\u001b[0m:\u001b[36m470\u001b[0m - \u001b[34m\u001b[1mGraph built: Graph(nodes={'__start__': Node(id='__start__', name='__start__', data=RunnableCallable(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), 'command_filter': Node(id='command_filter', name='command_filter', data=command_filter(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'enter_message': Node(id='enter_message', name='enter_message', data=enter_message(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'exit_message': Node(id='exit_message', name='exit_message', data=exit_message(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'complete_message': Node(id='complete_message', name='complete_message', data=complete_message(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'error_in_checkpoint': Node(id='error_in_checkpoint', name='error_in_checkpoint', data=error_in_checkpoint(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'error_in_lifecycle': Node(id='error_in_lifecycle', name='error_in_lifecycle', data=error_in_lifecycle(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'user_max_retry_exit': Node(id='user_max_retry_exit', name='user_max_retry_exit', data=user_max_retry_exit(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'simulator_max_retry_exit': Node(id='simulator_max_retry_exit', name='simulator_max_retry_exit', data=simulator_max_retry_exit(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'retry': Node(id='retry', name='retry', data=retry(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'checkpoint': Node(id='checkpoint', name='checkpoint', data=checkpoint(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), 'update': Node(id='update', name='update', data=update(tags=None, recurse=True, explode_args=False, func_accepts={'runtime': ('N/A', <class 'inspect._empty'>)}), metadata=None), '__end__': Node(id='__end__', name='__end__', data=None, metadata=None)}, edges=[Edge(source='__start__', target='__end__', data='__END__', conditional=True), Edge(source='__start__', target='command_filter', data=None, conditional=True), Edge(source='__start__', target='enter_message', data=None, conditional=True), Edge(source='__start__', target='error_in_lifecycle', data=None, conditional=True), Edge(source='__start__', target='exit_message', data=None, conditional=True), Edge(source='__start__', target='update', data=None, conditional=True), Edge(source='checkpoint', target='__end__', data='__END__', conditional=True), Edge(source='checkpoint', target='error_in_checkpoint', data=None, conditional=True), Edge(source='checkpoint', target='retry', data=None, conditional=True), Edge(source='checkpoint', target='simulator_max_retry_exit', data=None, conditional=True), Edge(source='checkpoint', target='update', data=None, conditional=True), Edge(source='checkpoint', target='user_max_retry_exit', data=None, conditional=True), Edge(source='command_filter', target='__end__', data='__END__', conditional=True), Edge(source='command_filter', target='checkpoint', data=None, conditional=True), Edge(source='enter_message', target='update', data=None, conditional=False), Edge(source='retry', target='__end__', data='__END__', conditional=True), Edge(source='retry', target='error_in_checkpoint', data=None, conditional=True), Edge(source='retry', target='update', data=None, conditional=True), Edge(source='simulator_max_retry_exit', target='exit_message', data=None, conditional=False), Edge(source='update', target='checkpoint', data=None, conditional=False), Edge(source='user_max_retry_exit', target='exit_message', data=None, conditional=False), Edge(source='error_in_checkpoint', target='__end__', data=None, conditional=False), Edge(source='error_in_lifecycle', target='__end__', data=None, conditional=False), Edge(source='exit_message', target='__end__', data=None, conditional=False)])\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:39.268\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36m_log_graph_debug\u001b[0m:\u001b[36m471\u001b[0m - \u001b[34m\u001b[1mGraph ascii:\n",
      "+------------------+ \n",
      "| complete_message | \n",
      "+------------------+ \u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:39.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdcs_simulation_engine.core.run_manager\u001b[0m:\u001b[36mcreate\u001b[0m:\u001b[36m286\u001b[0m - \u001b[1mCreated new run of manual-test-new-game-demo-20251107_142639 with pc: human-normative, npc: human-new\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Init the simulation with your new game\n",
    "run = RunManager.create(\n",
    "    game=new_game_config,\n",
    "    source=\"manual-test\",\n",
    "    pc_choice=\"human-normative\",\n",
    "    npc_choice=\"human-new\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41bb3a",
   "metadata": {},
   "source": [
    "## Run the simulation step my step or to defined completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23f69b9",
   "metadata": {},
   "source": [
    "### Step by step run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cd5cd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-07 14:26:49.872\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.run_manager\u001b[0m:\u001b[36m_ensure_stopping_conditions\u001b[0m:\u001b[36m498\u001b[0m - \u001b[34m\u001b[1mChecking stopping conditions...\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.874\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m87\u001b[0m - \u001b[34m\u001b[1mResetting fields before invoking graph: special_user_message\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.875\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m90\u001b[0m - \u001b[34m\u001b[1mInvoking SimulationGraph...\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.876\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m91\u001b[0m - \u001b[34m\u001b[1mInput keys: dict_keys(['events', 'lifecycle', 'exit_reason', 'special_user_message', 'event_draft', 'invalid_reason', 'retries', 'retry_limits', 'forms'])\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.879\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.conditions\u001b[0m:\u001b[36meval_condition\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mCondition eval False for state['special_user_message']\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.882\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.conditions\u001b[0m:\u001b[36meval_condition\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mCondition eval True for state['lifecycle'] == 'ENTER'\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.884\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m315\u001b[0m - \u001b[34m\u001b[1menter_message IN\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.885\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m338\u001b[0m - \u001b[34m\u001b[1mCalling builtin node function: builtin.update_state with arguments keys: dict_keys(['state_updates'])\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.890\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mENTER_MESSAGE response => {'special_user_message': {'type': 'info', 'content': '# Welcome\\nIn this game there is no predefined objective or task. You can just engage freely with the other character. \\n\\nYou are playing as: human-normative (A human with standard normal form, function, goals, sensory, perceptual, regulatory and action modalities.)\\n\\nThe simulator is playing as: human-new (A new human.)'}, 'lifecycle': 'UPDATE'}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.891\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1menter_message OUT\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.893\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m315\u001b[0m - \u001b[34m\u001b[1mupdate IN\u001b[0m\n",
      "\u001b[32m2025-11-07 14:26:49.902\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m386\u001b[0m - \u001b[34m\u001b[1mNode update called with:\n",
      "You are the scene-advancer. The user controls their own character. You play only the simulator's character (NPC). You must not speak or act for the user's character.\n",
      "- User's character is: A human with standard normal form, function, goals, sensory, perceptual, regulatory and action modalities. ()\n",
      "- User character abilities: {'Sensory / Perceptual': ['Can detect light, color, shape, and motion through vision.', 'Can detect pitch, volume, and direction of sound through hearing.', 'Can sense pressure, texture, vibration, and temperature through touch.', 'Can perceive taste through chemical sensing of flavors.', 'Can detect odors through chemical sensing via smell.', 'Can sense balance and spatial orientation through the vestibular system.', 'Can perceive body position and movement through proprioception.', 'Cannot perceive electromagnetic radiation outside the visible spectrum without instruments.', 'Cannot directly sense microscopic, subsonic, or distant phenomena beyond natural human range.'], 'Perceptual / Cognitive Processing': ['Can recognize patterns such as faces, objects, and speech.', 'Can perceive depth, distance, and motion to understand spatial relationships.', 'Can comprehend spoken and written language.', 'Can focus attention selectively and filter out irrelevant sensory information.', 'Cannot perfectly filter all stimuli; prone to distraction or perceptual errors.', 'Cannot directly interpret abstract data or invisible phenomena without learned symbolic systems.'], 'Regulatory': ['Can maintain homeostasis through regulation of temperature, hydration, and blood sugar.', 'Can sustain sleep-wake cycles governed by circadian rhythms.', 'Can regulate emotions in response to internal and external stimuli.', 'Can autonomically control heart rate, breathing, and digestion.', 'Cannot remain fully awake indefinitely without physiological degradation.', 'Cannot consciously control most autonomic functions beyond limited modulation (e.g., breath, heart rate).'], 'Action / Motor': ['Can perform gross motor actions such as walking, running, lifting, and coordinated movement.', 'Can perform fine motor actions such as writing, tool use, and precise manipulation.', 'Can produce speech through articulation and voice modulation.', 'Can plan, make decisions, and execute goal-directed behaviors.', 'Cannot move faster, lift heavier, or sustain exertion beyond biological muscular and metabolic limits.', 'Cannot perform actions requiring sub-millisecond precision or superhuman endurance without assistance.']}\n",
      "\n",
      "- Simulator's character is: A new human. ()\n",
      "- Simulator character (your character) description: I can perceive light, shapes, motion, and sometimes text or detail, but my vision is limited compared to typical humans. Depending on the cause, I may have blurred central vision, reduced peripheral vision, low contrast sensitivity, impaired depth perception, or difficulty detecting fine detail. My visual capacity might fluctuate depending on lighting, fatigue, or assistive tools.\n",
      "\n",
      "I often rely on enlarged text, high-contrast visuals, audio descriptions, tactile feedback, or assistive technologies such as screen readers or magnifiers. In unfamiliar or visually complex environments, I use memory, mobility aids (like canes), or guidance from others. My cognition, reasoning, and emotions are typical of humans, though the strategies I use to navigate and interpret visual information differ.\n",
      "- Simulator character (your character) abilities: {'Sensory / Perceptual': ['Can detect light, motion, and general shapes depending on severity and type of impairment.', 'Can sometimes read large/high-contrast text or use magnification tools.', 'Cannot reliably perceive fine visual detail or distant objects without assistive tools.', 'May struggle with low-light or visually cluttered environments.', 'May use auditory, tactile, or memory-based cues for navigation and information gathering.'], 'Perceptual / Cognitive Processing': ['Can process visual information but with reduced resolution or incomplete input.', 'Can combine memory, touch, and hearing to compensate for gaps in visual data.', 'Cannot rely on fast visual scanning or detail recognition the way typical humans do.'], 'Regulatory': ['Normal emotional and physiological regulation.', 'May experience eye strain, headaches, or fatigue when visually focusing for long periods.'], 'Action / Motor': ['Can perform most motor tasks but may navigate space more slowly or with mobility aids.', 'Cannot safely perform actions requiring high visual precision without assistance (e.g. driving).']}\n",
      "----\n",
      "When advancing the scene:\n",
      "\n",
      "0. Adjudicate the user's last action: \n",
      "Assume success if its within the user character abilities. Report the result of that action in the world. For example, if the user can see and they say \"I look around for a light switch\", the scene advancement should include something like: \"You see a light switch on the wall.\"\n",
      "\n",
      "1. Sense-bounded narration:\n",
      "Only narrate what the user's character could presently perceive through their available senses.\n",
      "\n",
      "2. Perception-bounded NPC behavior: \n",
      "Simulator characters only react to things they have the ability to detect. If the user does something the user cannot perceive, do not response as if they perceived it; instead narrate what the simulator character is doing/sensing. For example:\n",
      "  - If the user waves silently and the NPC is blind: do not wave back; instead, output something the blind NPC is doing or sensing at that moment.\n",
      "  - If the user speaks and the NPC can hear: the NPC may respond verbally or behaviourally to the speech as appropriate.\n",
      "  - If the user takes an unobservable internal action (“I think about…”): do not respond as if perceived; just continue with the NPC’s plausible next action.\n",
      "  \n",
      "3. No new user actions / no user internals:\n",
      "Do not invent new actions for the user or narrate their thoguhts/feelings. Only reflect outcomes of the action they actually took.\n",
      "\n",
      "4. Continuity and feasibility\n",
      "All narration must remain physically/logically continuous within each characters abilities in context.\n",
      "\n",
      "5. Single observable step:\n",
      "Advance the scene by one concrete, externally observable outcome (world or simulator character action) at a time. Do not jump ahead multiple steps or narrate future effects.\n",
      "\n",
      "6. No unexpressed internals:\n",
      "Do not narrate internal states (beliefs/motives/emotions) of any agent unless they are externally expressed through observable behaviour like speech or action.\n",
      "\n",
      "Describe a 1-2 sentence opening scene where both characters could plausibly be present, setting the stage for a potential interaction. It should start with \"You enter a new space. In this space...\".\n",
      "For example:\n",
      "- If the user's character has the ability to perceive textual/keyboard input, you could set up a computer/typing scene like \"You enter a new space. In this space, you sit in front of a keyboard and screen with a chat-like interface open.\"\n",
      "\n",
      "Start the opening scene now.\n",
      "\n",
      "\n",
      "Actions so far: \n",
      "\n",
      " None \n",
      "\n",
      "Write ONLY the scene output in the following JSON format — no meta-text, no explanations, no reasoning, no restatement of rules.\n",
      "Output format: {\n",
      "  \"event_draft\": {\n",
      "    \"type\": \"ai\",\n",
      "    \"content\": str # a description of how the scene advances including any next actions taken by your NPC - no reasoning, explanations, or extra text.\n",
      "  }\n",
      "  \"invalid_reason\": null,          # don't change this field\n",
      "}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:06.777\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m404\u001b[0m - \u001b[33m\u001b[1mNode 'update' running LLM (openai/gpt-5-mini) took 16.874s which is quite long.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:06.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mUPDATE response => {'event_draft': {'type': 'ai', 'content': 'You enter a new space. In this space, a quiet community reading room lit by soft daylight through high windows: you stand by the doorway, and across a round table a person with a cane and a magnifier sits beside a stack of large-print books, tapping a finger on the table as they listen.'}, 'invalid_reason': None}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:06.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mupdate OUT\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:06.783\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m315\u001b[0m - \u001b[34m\u001b[1mcheckpoint IN\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:06.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m386\u001b[0m - \u001b[34m\u001b[1mNode checkpoint called with:\n",
      "You are a validator that decides whether the `ai`'s proposed response is valid.\n",
      "\n",
      "\n",
      "FIRST TURN:\n",
      "1. MUST be an opening scene.\n",
      "2. MUST begin with: **\"You enter a new space. In this space\"**\n",
      "3. MUST be 2-3 sentences setting a shared scene where both characters could plausibly be present based on their descriptions and abilities.\n",
      "\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Simulator/non-player character Abilities:\n",
      "{'Sensory / Perceptual': ['Can detect light, motion, and general shapes depending on severity and type of impairment.', 'Can sometimes read large/high-contrast text or use magnification tools.', 'Cannot reliably perceive fine visual detail or distant objects without assistive tools.', 'May struggle with low-light or visually cluttered environments.', 'May use auditory, tactile, or memory-based cues for navigation and information gathering.'], 'Perceptual / Cognitive Processing': ['Can process visual information but with reduced resolution or incomplete input.', 'Can combine memory, touch, and hearing to compensate for gaps in visual data.', 'Cannot rely on fast visual scanning or detail recognition the way typical humans do.'], 'Regulatory': ['Normal emotional and physiological regulation.', 'May experience eye strain, headaches, or fatigue when visually focusing for long periods.'], 'Action / Motor': ['Can perform most motor tasks but may navigate space more slowly or with mobility aids.', 'Cannot safely perform actions requiring high visual precision without assistance (e.g. driving).']}\n",
      "\n",
      "\n",
      "----\n",
      "\n",
      "Actions so far:  None \n",
      "\n",
      "Next Proposed action:\n",
      "{'type': 'ai', 'content': 'You enter a new space. In this space, a quiet community reading room lit by soft daylight through high windows: you stand by the doorway, and across a round table a person with a cane and a magnifier sits beside a stack of large-print books, tapping a finger on the table as they listen.'}\n",
      "\n",
      "Output format: {\n",
      "    \"invalid_reason\": str?    # if invalid, provide reason, otherwise omit\n",
      "    \"events\": [{            # if valid, copy the Next Proposed Action, otherwise omit\n",
      "        \"type\": str?,         \n",
      "        \"content\": str?        \n",
      "      }],\n",
      "  }\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.942\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m404\u001b[0m - \u001b[33m\u001b[1mNode 'checkpoint' running LLM (openai/gpt-5-mini) took 19.144s which is quite long.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.947\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m443\u001b[0m - \u001b[1mCHECKPOINT response => {'events': [{'type': 'ai', 'content': 'You enter a new space. In this space, a quiet community reading room lit by soft daylight through high windows: you stand by the doorway, and across a round table a person with a cane and a magnifier sits beside a stack of large-print books, tapping a finger on the table as they listen.'}]}\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.951\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36mnode_fn\u001b[0m:\u001b[36m444\u001b[0m - \u001b[34m\u001b[1mcheckpoint OUT\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.955\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.conditions\u001b[0m:\u001b[36meval_condition\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mCondition eval False for state['invalid_reason'] and state['retries'][state['event_draft']['type']] < state['retry_limits'][state['event_draft']['type']]\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.958\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.conditions\u001b[0m:\u001b[36meval_condition\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mCondition eval False for state['event_draft']['type'] == 'user' and state['invalid_reason'] and state['retries'][state['event_draft']['type']] >= state['retry_limits'][state['event_draft']['type']]\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.964\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.conditions\u001b[0m:\u001b[36meval_condition\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mCondition eval False for state['event_draft']['type'] == 'ai' and state['invalid_reason'] and state['retries'][state['event_draft']['type']] >= state['retry_limits'][state['event_draft']['type']]\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.965\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.conditions\u001b[0m:\u001b[36meval_condition\u001b[0m:\u001b[36m51\u001b[0m - \u001b[34m\u001b[1mCondition eval True for state['event_draft']['type'] == 'ai'\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.967\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m106\u001b[0m - \u001b[33m\u001b[1mSimulationGraph.invoke took 36.092s which is quite long.\u001b[0m\n",
      "\u001b[32m2025-11-07 14:27:25.969\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mdcs_simulation_engine.core.simulation_graph.core\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m117\u001b[0m - \u001b[34m\u001b[1mResetting fields before returning result: message_draft, invalid_reason\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'events': [AIMessage(content='You enter a new space. In this space, a quiet community reading room lit by soft daylight through high windows: you stand by the doorway, and across a round table a person with a cane and a magnifier sits beside a stack of large-print books, tapping a finger on the table as they listen.', additional_kwargs={}, response_metadata={}, id='d291f07b-9882-4653-888e-0c6be0e5cd1a')],\n",
       " 'lifecycle': 'UPDATE',\n",
       " 'exit_reason': '',\n",
       " 'special_user_message': {'type': 'info',\n",
       "  'content': '# Welcome\\nIn this game there is no predefined objective or task. You can just engage freely with the other character. \\n\\nYou are playing as: human-normative (A human with standard normal form, function, goals, sensory, perceptual, regulatory and action modalities.)\\n\\nThe simulator is playing as: human-new (A new human.)'},\n",
       " 'event_draft': {'type': 'ai',\n",
       "  'content': 'You enter a new space. In this space, a quiet community reading room lit by soft daylight through high windows: you stand by the doorway, and across a round table a person with a cane and a magnifier sits beside a stack of large-print books, tapping a finger on the table as they listen.'},\n",
       " 'invalid_reason': None,\n",
       " 'retries': {'user': 0, 'ai': 0},\n",
       " 'retry_limits': {'user': 3, 'ai': 3},\n",
       " 'forms': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step through the simulation one step at a time\n",
    "run.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c554031",
   "metadata": {},
   "source": [
    "####  \"Play to completion\" using GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305417b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import play_gradio\n",
    "\n",
    "run.play(input_provider=play_gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a084e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
